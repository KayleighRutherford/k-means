import os
import numpy as np

#this algorithm implements k means clustering on a set of data and produces a file with the cluster means and the members (data values) in each cluster
#modify this source code in line 90 to specify the file containing the data you want to cluster, and the number of clusters k
#run kmeans.py in the terminal


def kmeans(data, k): #takes a tab delimited file of data for clustering, and a number of clusters k
	with open(data, "rb") as f:
		d=np.loadtxt(f,delimiter="\t")
		d=d[0:,0:-1] #in the case of data generated by generate-clusters.py, the last column is an index and must not be stored when clustering (take this line out for data from other origins)
		
		
		centres= []

		centres = randomize_centres(d, centres, k)  #randomly places centres amongst the data to begin

		old_centres = [[] for i in range(k)] 

		iterations = 0
		while not (has_converged(centres, old_centres, iterations)): #runs until the next iteration yields the same centres as the previous iteration
			iterations += 1

			clusters = [[] for i in range(k)]

			clusters = euclidean_dist(d, centres, clusters) #finds distances between data and centres and assigns each data pt to its closest centre to build clusters

			# shift centres according to calculated distances
			index = 0
			for cluster in clusters:
				old_centres[index] = centres[index]
				centres[index] = np.mean(cluster, axis=0).tolist() #places each centre at the mean value of a cluster
				index += 1

		f=open("kmeansoutput.txt","w")
		print>>f, "The total number of data instances is: " + str(len(d))
		print>>f,"The total number of iterations necessary is: " + str(iterations)
		print>>f,"The means of each cluster are: " + str(centres)
		print>>f,"The clusters are as follows:"
		for cluster in clusters:
			print>>f,"Cluster with a size of " + str(len(cluster)) + " starts here:"
			print>>f,np.array(cluster).tolist()
			print>>f,"Cluster ends here."

		return
		
		f.close()
   
def euclidean_dist(d, centres, clusters):
    for point in d:  
        mu_index = min([(i[0], np.linalg.norm(point-centres[i[0]])) \
                            for i in enumerate(centres)], key=lambda t:t[1])[0] #finds the distance between a point and a given centre using vector normal, and assigns it to the closes centre by minimising this distance.
        try:
            clusters[mu_index].append(point) #add point to cluster
        except KeyError:
            clusters[mu_index] = [point]
       
    for cluster in clusters:
        if not cluster: #if there is an empty cluster
            cluster.append(d[np.random.randint(0, len(d), size=1)].flatten().tolist()) #randomly give it a data point so that it is not empty

    return clusters

def randomize_centres(d, centres, k):
    for cluster in range(0, k):
        centres.append((d[np.random.randint(0, len(d), size=1)]+0.0001).flatten().tolist()) #initialises centres to a random point in the data set, and offsets it by a very small amount (to avoid zeros in calculations later in the problem set)
    return centres

   
def has_converged(centres, old_centres, iterations):
    MAX_ITERATIONS = 1000
    if iterations > MAX_ITERATIONS:
        return True
    return old_centres == centres #gives a value of true if an iteration did not shift the centres to a new/improved value
	
def main():
	kmeans("test_data.txt",3)
	
if __name__ == "__main__":
    main()